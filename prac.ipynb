{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import ale_py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "# env = gym.make(\"Enduro-v4\")\n",
    "# state, _ = env.reset()\n",
    "# plt.imshow(state)\n",
    "\n",
    "# state.shape\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_env(env):\n",
    "    # env = AtariPreprocessing(env, screen_size=(84, 84),frame_skip=1, grayscale_obs=True)\n",
    "    env = AtariPreprocessing(env, frame_skip=1, grayscale_obs=True)\n",
    "    env = FrameStackObservation(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAAC8CAYAAAAQL7MCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApwElEQVR4nO3da28bZ37+8WvOB54pUrYse73JZtNtsrsoNkCBtkBRoH3UvssC/9exD4puUQSLNk2zWSRx4liWIkuURA7nfPw/UOe2JFvmyOJhSF4fwHBiS/KQnq+Bn+6Ze6SiKAoQERERERHNmbzqAyAiIiIios3EYYOIiIiIiBaCwwYRERERES0Ehw0iIiIiIloIDhtERERERLQQHDaIiIiIiGghOGwQEREREdFCcNggIiIiIqKF4LBBREREREQLoVb9QEmSbv09TdPw9OlTmKYJ0zQhy5xhaPn+8z//c9WHwE6o9tgJ0Wx16ARgK1R/VVrhmUlERERERAvBYYOIiIiIiBaCwwYRERERES0Ehw0iIiIiIloIDhtERERERLQQHDaIiIiIiGghOGwQEREREdFCcNggIiIiIqKF4LBBREREREQLwWGDiIiIiIgWgsMGEREREREtBIcNIiIiIiJaCA4bRERERES0EBw2iIiIiIhoIThsEBERERHRQnDYICIiIiKiheCwQUREREREC8Fhg4iIiIiIFoLDBhERERERLQSHDSIiIiIiWggOG0REREREtBAcNoiIiIiIaCE4bBARERER0UJw2CAiIiIiooXgsEFERERERAvBYYOIiIiIiBaCwwYRERERES0Ehw0iIiIiIloIDhtERERERLQQHDaIiIiIiGghOGwQEREREdFCcNggIiIiIqKF4LBBREREREQLoa76ABZFkqRrPyuKIv4bAIqiQFEUyPNc/P/Vn69+7FXl72+6q++fLL+eSW++X1dty3uzSdjJ/bCT7cBO7oedbA+2cj+b2spGDhuSJMGyLOi6jn6/D8uyMBgMYNu2OPEdx0EQBDg7O4Pv+3BdF1EUIU1T5HkOVVXFD1mWxa8nSYI0TVf9EhdKkiRomgbLsmDbNobDoQggDEOMRqNr/1gAQJIkiOMYWZZd+3WqL3ZyP+xkO7CT+2En24Ot3M8mt7Ixw0Y5AcqyDEmSYJomTNNEt9tFu93G3t4eWq0WVFWFJEk4OzuD53nIsgyKooi/KFmWkWUZdF2HruvQNA2qqiKKImRZJv68q1PmOkyVV73tOw83f1/TNNi2jXa7jcFgID7W8zyEYYg8z6+9H0mSAACyLEMcxwDe/I4FrR47qY6dbC92Uh072W5spbptbmXth43yRC+n5+FwCNu20Wq1oOs6TNOEpmkwDENMygDQbrfRaDTQbreRpik8zxPTdZZlMAwDmqZB13UoiiImx8lkAt/3MR6P4TgOfN9HEASrfAvuRJZl6LoO27axs7MDy7LQ6XSuLV1KkgTDMMQ/EJZlid/LsgxPnjx5I/Q0TeG6LsIwxMXFBYIgwGg0QpIkiKKoVif9NmInd8NOthM7uRt2sr3Yyt1seytrPWyUf0nlX0qr1cLOzg7a7TaazSZ0XX/jWsFyIlQUBYqiQNd1AIBlWWKZLs9zEUh5wpe/p+s6ptMp0jRFmqaI41j8GXX5S72NJEnidVuWJd6nq0t15cfpuo5Wq/XWr9NsNt/4tTRN0Ww24fs+iqKAoihwHAd5nq/N+7Op2MndsJPtxE7uhp1sL7ZyN2xljsPG1WWbZVw3Vk7KOzs7Ytkuz3Ocnp5iPB6Lv9iqsiwTE2NRFGJJsPy5fF1JkohlP8uykOc5TNOE67rwPO/er+vqzVQ3b6S6jzJe27bR6XSgaRriOIbjOIii6I2Pl2UZqlr99Cj/McmyTCz1dbtdxHGM8XiMJEnEr28zdsJO2Mls7ISdsJNq2ApbWYdW5rqysezpSZZlGIYB27YBXF7L5/v+0v58RVFgGIY4lnm7bVeG9yHLMhRFgaqqMAwDkiSJ7xCEYTi3P+eq8jsXqqqu/ESvE3YyX+xkM7GT+WInm4utzBdbmb+1vYyqnHRPT09xdna2suMop9/yxpz7KpcXy8n+5s1R7+vqjg7z+C5AVUVRXPuOBC0XO7kbdrKd2MndsJPtxVbuhq1cWuthA8BS//KWoVxCK0/4eW31luc58jxf6DRN9cNO7oadbCd2cjfsZHuxlbthK5fWdtjYNLIsi+W2q9c7SpIEVVWv3WBFtK3YCdFs7ISoGrayHPO/2I3eS3mNYHlTFHC5DFae8PO8hpBoXbETotnYCVE1bGU5uLJRE5qmwTRNZFkG3/fFUmW5tVy5DzXRNmMnRLOxE6Jq2MpycGWjJhRFgaZpAIA4jpGmqXgypKqqd9pKjmhTsROi2dgJUTVsZTm4srFi5fWCV5fvru4ckCSJ2Gu62WyKp0ISbRN2QjQbOyGqhq0sF4eNFStP9PKkvnnCZ1mGJEmgqio0TavFFmZEy8ZOiGZjJ0TVsJXl4mVUK6ZpGhqNBoqigO/7YvmuVG6ZBnBJj7YXOyGajZ0QVcNWlosrGytW7u2cZRniOH7jSaDlHs1FUfBkp63FTohmYydE1bCV5eKwsSKqqkLXdciyLG5KetcyXXmtoCRJaLVaSJJkqx8QQ9uBnRDNxk6IqmErq8HLqFakXJaTJElM0OWPt8nzHGEYoigKmKYJVeWcSJuPnRDNxk6IqmErq8F3bcnKE71clkuSBGmavrGEd1Oe55Bl+doj723bRpqmiON44cdNtEzshGg2dkJUDVtZLQ4bSybLspiqgcvt1qo+MCbPc2RZhizLIMsydF3nDgm0kdgJ0WzshKgatrJavIxqicobkizLAoD3ejJlnufiZiZFUcS0Lsv8q6TNwE6IZmMnRNWwldXju7Qk5QlZnqTA5dMqsyy709cpt2PLsgySJF37QbTu2AnRbOyEqBq2Ug8cNpZE13U0Gg0AgOd5977WL01T+L4vnm5pGMY8DpNopdgJ0WzshKgatlIPHDaWRFEUmKaJoigQhuGdp+qb8jxHFEUoigKapnEfaNoI7IRoNnZCVA1bqQfeIL5g5e4HkiQhy7J3brF2F+UOCVmWiSdfmqYpriskWifshGg2dkJUDVupF65sLJgkSZBlGZIkoSiKmXs630V5DWF5wmuaxpuVaC2xE6LZ2AlRNWylXvjuLIgsy2KLNMuyUBQFfN+/8w4IsxRFgSiKkOc5DMPgSU9rhZ0QzcZOiKphK/XEd2aByn2dVVUVJ+a8T/gsy8SDacpt2MqJnmgdsBOi2dgJUTVspX74riyIpmkwTRMAEIbh3E/0m+I4xnQ6RZZlaDabME2TJz3VHjshmo2dEFXDVuqJ78iCKIoCXddRFMV77el8V+V2bHmewzRNaJrGKZtqj50QzcZOiKphK/XEd2POyqU7WZbFTUnlTgjLkGUZ4jhGnufQNE08xIaoTtgJ0WzshKgatlJvHDbmTJIkqKoqnipZ7lqwrBO+KAqxbMgblqiu2AnRbOyEqBq2Um98N+bk6g4I5Z7LQRAs/HrBm7IsQxRFyLIMqqpCVVVx8xLRqrETotnYCVE1bGU9cJ1njsodEDRNQ5IkCMNw6ceQZRmyLIMsy1BVFXmei0mfqA7YCdFs7ISoGrZSfxw25uTqNXrL2AFhljRNEQQBJEmCZVlIkmThN0oRzcJOiGZjJ0TVsJX1wPWdObm6A0IdTq48zxGG4bUdEohWjZ0QzcZOiKphK+uBKxv3pOu62NfZMAykabrUm5LepSgKAJcxGoYBACtbYqTtxk6IZmMnRNWwlfXClY170jQNhmGIH7Is1+KEL7d9A15P/o1GQ5z4RMvETohmYydE1bCV9cJh454sy0K320We5xiPx4iiaNWHdE0URXAcB0mSwLZtmKYJXde5BzQtFTshmo2dEFXDVtYLh417MgwDjUYDRVFgOp0iSZJVH9I1aZqK4zIMQ5zs3I6NlomdEM3GToiqYSvrZTtHrDloNptiT+fxeAzf91EUhbhWr27iOMZkMgEAdDodhGGIOI5XfFS06dgJ0WzshKgatrKetnPEmgPLstDr9VAUBcbjMeI4RpZlK79e8DZxHGM6nSLLMrTbbViWtbUTNi0POyGajZ0QVcNW1hNXNu6ofDKkqqpiq7XyZK+zoigQRRFUVRXLjY1GY+t3SKDFYCdEs7ETomrYynrjsHFHuq5D13XIsoyiKMQj6uuufLqlrutIkgSSJME0TciyvFUnPC0HOyGajZ0QVcNW1tv2reXc09UdECaTydqdLHEcw3EcxHGMVqsFy7KgqioURVn1odEGYSdEs7ETomrYynrjsHFHlmWh1Wohz3Oxrdk6KW9WStNUbMemqiokSVr1odEGYSdEs7ETomrYynrjZVQVtVotmKYJAHAcB2EYIsuy2u6AMEscxxiPxwCAwWCAIAhwcXFR25usaD2wE6LZ2AlRNWxlM3Blo6JWq4XhcAgAmE6niKKo1jsgzBJFESaTCbIsw87ODprN5qoPiTYAOyGajZ0QVcNWNgOHjRlUVYVpmjAMQ+zt7Ps+0jRd9aHdS1EUiOMYSZIgyzKoqopOp4NGo7HqQ6M1xE6IZmMnRNWwlc3CYWMGVVVhGMa1Ez4Mw7U/4cudHG6e8LZtb+Ue0HQ/7IRoNnZCVA1b2Syb+8rmQJZl9Pt9PHr0CEmS4PDwEJ7nrfqw5iqKIpyfnyOKIvR6va1Z0qP5YSdEs7ETomrYyubhsHELWZYhSRJ6vR729/cRxzFevny5cSd8HMc4OztDHMfihN+W3RHo/tgJ0WzshKgatrKZuBvVLRqNBmzbRpqmGI1GCMMQRVGs7U1J71IUBXzfx+HhIfI8x6NHjxCGIc7Ozjby9dL8sBN2QrOxE3ZC1bCVzWyFw8YtOp0Oer0ePM+D67oIgmDj/vIBiNfkui6SJEGv18PTp09xfn6O8/PzFR8d1R07YSc0GzthJ1QNW9nMVngZ1Q2Koogbk0zTRJIkcF0XcRyv+tAWqigKJEmCKIoQRREAiAfPEN3ETtgJzcZO2AlVw1Y2uxWubNxw9YQvl/Imk8mqD2vhsixDlmXipC+KAu12G0EQII7jjfzOAr0/dsJOaDZ2wk6oGray2a1wZeP/yLIMRVHQ7/exv7+PKIpwcHAA13VXfWhLFYYhRqMRoihCv99Hu91e9SFRjbCTS+yE3oWdXGInNAtbubTprXDY+D+SJEFRFAyHQzx9+hRhGOL777/HdDpd9aEtVRzHODk5QRiGePDgAbrd7kbvkEB3w04usRN6F3ZyiZ3QLGzl0qa3wsuo/k95jVyaphiPxxt/neBt8jyHLMsIggDHx8dIkgR7e3sIggAXFxcbs6RH74edXGIn9C7s5BI7oVnYyqVNb4UrG7hcxms2m+j3+4jjWGy3tq3yPIfv+zg4OIDv+3jy5AmGw+FGTdl0d+zkOnZCb8NOrmMndBu2ct0mt7L1w4aqqtB1HZZlwbZtxHEMx3GQJMmqD22lsixDFEVIkgRpmkJRFDSbzY3bIYGqYSdvx07oKnbyduyEbmIrb7eprWz9sFHufNBoNNBsNpEkCc7Pz7d6ugYuT/gwDBFFkVjea7VaaDabkOWtP222Djt5O3ZCV7GTt2MndBNbebtNbWWr79mQZRm7u7vo9XpIkmTrl/DeJooinJycwLZt/OxnP8NkMsF0OkWapsiybNWHR0vATmZjJ8ROZmMnBLCVKjatlfUdk+ZAkiTs7+/jL//yL1EUBY6OjuD7/qoPq1bK6wfDMMQvf/lLPHnyBLquQ1GUVR8aLQk7mY2dEDuZjZ0QwFaq2LRWtnZlo9frwbZtZFmG8/NzeJ4nlq3oTWEY4ujoCGEY4uHDh/A8D69evVrLCZuqYyd3w062Ezu5G3ayvdjK3WxKK1u5siHLMgaDAZ4+fYokSfDTTz/BcRyEYYg0TVd9eLXkeR6ePXsGx3Hw4Ycf4tGjRxuxQwLdjp3cHTvZPuzk7tjJdmIrd7cprWzdyoZhGNA0Dc1mE7ZtYzKZYDKZ8HrBGZIkgeM4UBTl2i4SSZLwvdtA7OT9sJPtwk7eDzvZPmzl/WxKK1s3bFiWBcuy0G630el08O233+Lly5erPqzai6IIURRB13WoqgrTNNHpdOD7PuI45hLohmEn74edbBd28n7YyfZhK+9nU1rZqmFDlmX0ej30ej1xHRxvSrobz/Pw/PlzqKqK/f19OI4Dz/PWdocEehM7uT92svnYyf2xk+3AVu5v3VvZqns2JEnC3t4efvGLX8DzPHz33XeYTqerPqy1MplM8MUXX+Dk5AR/8Rd/gQ8++EBM3LQZ2Mn9sZPNx07uj51sB7Zyf+veynoc5T2VD0WxLAtFUcB1Xfi+z5uS7iGOY5ycnCAMQ+zu7sLzPJyenq7FhE1vx07mj51sHnYyf+xkM7GV+VvXVrZi2ACA4XCIXq+HNE0xGo3gOA4n63vwfR8//PADbNvG06dPcXFxgdFotOrDontiJ/PFTjYTO5kvdrK52Mp8rWsrG38ZVXn3frPZRKfTQRiGOD09Xau7+OsoSRJMJhO4rgtFUcR7bNs2ZHnjT6uNw04Wg51sFnayGOxk87CVxVjXVjZ+ZcMwDBiGgX6/j8FggO+++w7Pnz9f9WGtvTAMcXx8jCzLoKoqLMtCv98XD+ih9cJOFoOdbBZ2shjsZPOwlcVY11Y2etiQZRn9fh+dTgdBEODw8BCe5636sDZKHMc4Pj6GqqrY3d3FZDLB2dnZqg+L7oCdLB47WX/sZPHYyWZgK4u3bq3Ud81lDmRZxtOnT/HJJ5/AcRx8/fXXcBxn1Ye1UTzPw7fffovz83N8+OGHePz4MRRFWfVh0R2wk8VjJ+uPnSweO9kMbGXx1q2VjVzZkGUZjUYDtm0jz3O4rosgCBCGYe3v2F83RVEgSRIEQSCux9zd3UUQBLi4uOD7XWPsZHnYyfpiJ8vDTtYbW1medWtlI4cNSZLw4MEDdLtdZFmG09NTcUMNzVeWZciyDJPJBD/88AMsy8LHH3+M8XiM8Xi86sOjd2Any8NO1hc7WR52st7YyvKsWysbO2y0Wi30ej2cnp7CdV3EcbzqwwIAmKYJ0zTF/xdFgTzPYZombNtGkiRIkgRxHCOKIvHxnufVeru4NE0xmUwgSRJkWYau62g0GoiiCHEcI8/zVR8i3cBOlo+drB92snzsZD2xleVbl1Y2ctgAgN3dXTx+/BjPnz/HixcvavMAmU6ng8FgIP4/TVOkaYrd3V08fPgQ0+kU0+kUk8kEFxcXYieHo6OjWp/wURTh9PQUAKDrOizLQq/Xg+d5OD8/X/HR0W3YyXKxk/XETpaLnawvtrJc69LKRg0bsizj4cOHaDQacF0Xz58/h+u6tbh2TVVVqKqKdruNBw8ewLZtNBoNAJffDShPbM/z4HkeXNeF4ziwLAuNRgNhGOLZs2crfhWzJUmCk5MTyLKMwWAAwzBwcXGx6sOiK9jJ6rGT+mMnq8dO1gNbWb26t7Jxw8Ynn3yCR48e4T/+4z9wdHSEKIpqccKbpolGo4HHjx/j448/xpMnT/D48WOYpilO6maziSiKxI8gCBBFEcIwhOu6+Pzzz1f9MmZyXRfffvstBoMBPvvsM5yfn+P58+e1+e4GsZM6YCf1x05Wj52sB7ayenVvZSO2vi13QOh0OsjzHL7vi+vviqJY9eEBgFjaMk0TRVEgiiK4ros0TaEoCmRZhizLUBQFiqJc22nAcRwoioKHDx+i1Wqt+qW8U1EUiOMYQRBgPB4jDEMMBgP0+/1ab8u2DdhJfbCT+mIn9cFO6o2t1EfdW9mIlY1yKazdbovr1zzPq82NSQDQ7Xbx5MkTtFot5HmOyWSCNE0hyzK63e4bHx9FERzHwfn5OUajERRFwSeffIKDg4NaXz+YZRl834csy3jx4gVM08RHH30Ex3Hw1Vdf1eI7HduKndQHO6kvdlIf7KTe2Ep91L2VtR82yom02+2i2+3CdV2EYYgwDFd9aNekaSr2mpYkCYZhoNVqodFowDRNaJoG4PL1aJoGTdOg6zqAy8fTy7KMVqsFwzBW+TIqK3dIyPMcu7u7SJJEvJ46/UO0LdhJPbGTemEn9cRO6oet1FNdW1n7y6jKE2R/fx8ffvghLi4u8M0339RuAo2iCNPpFHEcQ5IktNttPHz4EDs7O2i32+JEVlVVbMXWbDYBAI7jQJIkDIdD8Wt1F4Yhjo+PMR6P0Wg00Gq10Gw2YVkWZHntT7u1w07qiZ3UCzupJ3ZSP2ylnuraylqvbCiKguFwiEajAc/zcHx8jCAIVr5c9DZXr1+UJAlxHIv9mw3DgGmaMAxDbMc2nU5xdnYGz/Oufd46yfMcURTh+PhYPOzHdV14nlebvZ+3ATupN3ZSD+yk3thJfbCVeqtjK2s9bEiShI8++gjD4RBff/01RqMRHMdZ9WFVUm4LF0URfN/H7u4uHjx4gDAMMZ1OcXh4iO+//x6+76/6UO/FdV38z//8D4bDIf7mb/4G4/EYh4eHtdkhYRuwk/pjJ6vHTuqPndQDW6m/urUyl2Ejz3PxpMggCJY2BSqKgu+++w6np6d48eIFHMdBEAS1/YfH9304joPj42Poug5d19FsNtFsNtHpdNDtdhEEAYIgwOnpKV69eiWeZinLMlRVxYsXLzAej2uz08MskiRBVVWEYYhmswnP8zAajZAkyaoPbenYSTXshJ2wk9nYyXZ3ArCVqtjK6luRiorv3KyTeFXLTLIsQ5IkFEUhftSRJEnXftz89fK/AbzxWsqfJUlCnudrt2Rcvq7yesE8zxfy91SH94Wd3A87YSdVfn9R2En9bVMnAFu5L7ZSj1bmNmwQrVod/rFjJ1R37IRotjp0ArAVqr8qrXAbByIiIiIiWoi1vkH8fZT7KpdLgNuqKArkeY4kSWq3LzatHju5xE7oXdjJJXZCs7CVS9vaylYNG5IkYWdnB51OB7quQ1W36uVfk6Yp4jjGZDLB4eHhqg+HaoSdvMZO6Dbs5DV2Qu/CVl7b1la27m/cMAzYtn3tyZFXaZoGRVGgKEqtHxYUxzHCMESapkiSBJZlod1uIwgC8SAaWZbFg3duKqfqMAzFTV5EJXZyiZ3Qu7CTS+yEZmErl7a1la0aNiRJQrPZxM7Ojjjpb+p2uzBNE41Go9aPpx+NRjg6OsJ0OsV4PMaDBw/w6aef4sWLF/jf//1f8TRM0zTRbDbfWLYMwxC+76/08fVUT+zkNXZCt2Enr7ETehe28tq2trJVwwbwerszRVHe+D1ZlmGapth32bKsW7/OyckJzs/PYRgGVFVFEASI41gsFQZBgDAMkSSJeDJlnudotVpotVri60ynU0ynU7GXs6qq0DQNpmnCsiyMx2Ocn59D13VYloU0TcXDaIqiEK+lnJDL/y+KAkEQiN9722vd5usm6d3YyevXyk7oNuzk9WtlJ/QubOX1a93GVrZu2Jil1+vh4cOH2NvbQ7fbvfXjXrx4ga+++gq9Xg+tVgsvX77E6ekp/v7v/x6/+MUvcHBwgLOzM4zHYziOA9/3EQQBPv30U/T7ffF1jo+P8ac//QmWZcG2bXQ6HXQ6Hezv72N3dxc//vgj/vCHP2B3dxf7+/twHAfj8RiKoojpX9d1ABAP1dF1HZ7n4fz8HEVRXPvziOaBnRDNxk6IqmErm43Dxg3l9XbllPs2eZ4jiiJMJhPs7Oyg1+vh1atXYpLOsgxBEMB1XQBAo9FAnudiwvY8T3ytLMvENN1oNFAUBVzXRRAESJJE/NA0Db1eD57nYTKZoNlswrZtsfQIAK9evQIA/PznP8fx8bG4fpBo3tgJ0WzshKgatrLZ6nsXTs2Nx2McHh6i0+ng17/+NXZ2dpDnOcIwhOM4ODk5wcuXL6HrOj744APs7+9jMBggTVMcHx+LH3meYzAYYH9/Hx988AE0TROTuuM4CMMQeZ5jZ2cHv/71r9FqtXB4eAjXdWGaJj766CP8y7/8C3Z3d/H5558jz3P88z//M373u9/deiMW0bKwE6LZ2AlRNWxlPXFl4z3Zto1+vw9JkuA4DoqiEDc+maaJVquFXq8HVVURxzGSJAFwuSNDu90WXydJEriuiyzLEMcxVFUVy4PltYO2bQOAmJb7/T5s24YkSdA07drvp2kK27ah63qtd3Sg7cBOiGZjJ0TVsJX1xGHjPUiShJ/97Gf47LPPUBQFvvnmG8RxjOFwiN3dXezu7iLLMrRaLYxGI7x69QphGCKOY/T7fXz88cfia3399df46aef4DgOoihCv9/HkydPxNcZDocYDoeI4xjffPMNJEnCZ599hjzPURQFdF1Ho9FAHMdi6rZtG4ZhbO2NSFQP7IRoNnZCVA1bWV8cNm4ol+IMw0AURbd+XBzHMAwDkiQhz3Nx4qVpivF4jCAIkOc5JEmCqqriBCyKAr7vi69TFAUsy4KmaVBVVXy9MAwxmUyQZRkajQY0TRM7HhiGIa4nDIIAp6enSNMUvV4PkiRhNBohCAKYpiluYCKaJ3ZCNBs7IaqGrWw2qaj4RJFNmNJkWcZvfvMb7O3todlsvvWaumazCV3XZz7lsrzhqFQUBYqigKqqUBQFWZYBgJiCy48pH1pTyrIMWZaJ97e8SQqA+Dppmopt40phGMLzPNi2jXa7jdFohIODAwwGAzx58gSO4+D09BSKorz1dcZxDM/zcHR0hC+//HIjHixTh9fATq5jJ/VTh9fATq5jJ/VTl9fAVq5jK/VT5TVwZeOGJElQFMW1k/Au0jS91+9XlWUZiqJAkiTwPA95notrCT3PQ5IkUFWV1w7SQrATotnYCVE1bGWzcdi4oVwiA+r9HYWiKJDnubgBKssyNJtNAJcPqyknfaJFYCdEs7ETomrYymbjO3JDudQmy/JbJ9M8z8W+zW+blMvPK59IuSrlcWZZJgImmhd2QjQbOyGqhq1sNg4bV8iyjEajAdu2xRZmN/m+jziO4TiOeHDMVeWJ3uv10Ol0lnHYbxXHMXzfh+/7cBzn2jWORPfBTohmYydE1bCVzcdh44byaZKmaUJVVZycnMBxHDx+/Bg7OztQVRVBEFzb1eAqWZbFzUW+74st0K7ehFTuanB1Z4NySU5RFLGDgmmacBwHk8kEmqbBMAzxlMxWq4Vut4vxeIzRaCSOO4oiuK4LTdNgWdbcrlMkuoqdEM3GToiqYSubjXew3NDtdrG/v49+v49ms4k//vGP+H//7/9hOp3iV7/6FT744AM8evQIjUbjrZ9fnmjj8RjPnj3Djz/+iMPDQ7x8+RI//fQTXr16hdFoBAAiIN/38dNPP+Hrr7/GixcvcHZ2BgAYDocYj8f4t3/7N3z55Zc4Pj7GDz/8gC+++AK+7+Pjjz9Gmqb4/e9/jz/96U/wfR/ff/89fv/73+P58+fY2dkR1xISzRM7IZqNnRBVw1Y2G4eNG8rp2DRN2LaNPM8RBAGKooCmadeuK3yX8sQHIB57D1wu9ZVbu8myjDRN4Xke0jSFLMtiz2hN08T1iXEcQ1EUcfJOJhOEYQhFURDHMSaTCfI8R6/Xg67r4vdvbtlGNC/shGg2dkJUDVvZbLyM6ha9Xk9cQ3hXkiRhOBzCNE0cHBzg6OgIvV4Ppmmi2+2i0+lcO3lfvHiBRqOB4XCIR48e4fHjx0jTFI7jiAfUDIdD/Pa3v4Xrujg8PMTFxQXSNIXv+zg5OYFlWfjrv/5rnJ2d4eTkBOPxWCz7ES0KOyGajZ0QVcNWNhOHjVuUOwmUU3KWZQiCoPIJdHVHgvLhMOU2aeUezUEQII5j8eAYXdehKAokSRIPn9F1HaZpAric0mVZRrvdhmVZYhpvtVpQVRWu66IoCvH7Vx9SQ7QI7IRoNnZCVA1b2UwcNm5xcXGB8/Nz6LqOvb09pGmKw8PDSjsLFEUB13UxHo8xnU6Rpilc10WSJGJrNtd1IUkSHMdBlmVQVVVM8r7vo91uo91u48GDBxgOh4jjGH/+858hSRI+++wz/PznP4dhGNjb28Pvfvc76LqOzz//HNPpFH/1V3+Fp0+fznwSJ9F9sROi2dgJUTVsZTPx3bih3FFAkiRxreBgMADw+oEtAG7dP/nqRA28vn5Q13Xx6PqruxRIkiR2XwAup/rymkBZllEUBZrNJlRVFVO4ZVnI81ws57VaLSiKgiAIAADtdhuKoiAMQ8RxvJg3irYaOyGajZ0QVcNWNptUlH+Dsz5wA252kWUZv/nNb7C3t4dmsylOwKtarZY4OWVZFktt5aRaXovneR7CMHzj81VVFTc5GYaBPM9FJOXS2tXltSzLkOe5+PXyY8qfy+3Zyl+7+lp0XUeapoiiSHx+URTXlhvLgG+K4xie5+Ho6AhffvklKp4GtVaH18BO2End1eE1sBN2Und1eQ1sha3UXZXXsFUrG0VRIIoiBEEgrtO7KU1T8XCY8to9AGJyLU/4MoSbypM5iiIYhrGgVzJbea1iuaPCTXEci+sWia5iJ6+xE7oNO3mNndC7sJXXtrWVrRs2zs/PEYYhdF2/9Qae8tff9h2FcoKbdf3gzWl42WYdZ57nYsLehMma5oedvMZO6Dbs5DV2Qu/CVl7b1la2atgALqdeSZIQx/FW7xZQXt+4bdM1VcNOLrETehd2comd0Cxs5dK2trJV92wAr6/do0tXr2tcd3V4HexkM7GT+WInm4mdzB9b2Uzb1srKVjbKpTRd1+8dU/m0xnIXA+D1Ulr5tMjyL7b8s3zfF9f+5XmOVqsF0zTf+qaV26mV1w4SLQs7IZqNnRBVw1ZoFVYybCiKggcPHqDRaGBvb088OOU+TNOEoijiwS2NRgO6rqPb7cI0TYRhKG5AAoD//u//xsHBgbiO8G//9m/xy1/+Ujz85aqiKPDv//7v+POf/1xpr2eieWAnRLOxE6Jq2AqtykqGDUmS0Gg00O128eTJEzQajff+WuUUrWkaFEWB53lI0xTNZhO6rsOyLCiKIrZOK7dca7fbIoaiKPDgwQP0+31EUSQm7qsnd6/Xw2AwgKZpUFVVfFxRFCiKQmx/5roufN+/93tExE6IZmMnRNWwFVqVldyzYZom/umf/glPnjzBP/7jP2I4HL7X17m6VAdcXgP3zTff4OLiQjyM5ejoCNPpVJy85QNbTk9P4fs+fvvb32Jvbw+e5yGOY0ynU7FF29UbmXzfRxRFePr0KQaDAQ4ODvDq1SuxHZtlWTBNE3/4wx/wxRdfzOeNojupw/WP7ISd1B07uf2Y2AmV6tAJwFbYSv3V9p6N8kahcup92wNebvu88mfDMK4t3ZXTrqIoMAwDhmFA0zRIkoQsy8QUrigKJEnCzs4Oer0eNE0TuwKUXz/LMjQaDbTbbURRJB7sYts2+v0+BoMBXNdFHMfic5rNJizLQrPZXMA7RtuInRDNxk6IqmErtCorGTbyPIfjOBiNRvjxxx8xnU4rf2750JePPvoIlmXh2bNnOD8/x+HhIVzXRafTEY+gV1UVRVEgTVO0221xQ5Qsy/j0008xGAzwr//6r/jjH/+Iv/u7v8OHH34I13XhOA7+4R/+Ab/61a/w1Vdf4eDgQCzvlU+mbDQa2NnZwe7uLrrdLprNJmzbxn/9138t8J2jbcJOiGZjJ0TVsBValZUMG+UOA+PxWJyoVZXTuGEYaLVa+OGHHzAajXB0dIQgCLC7u4tGo4EkSaDrOi4uLjAej5FlmXiqpCRJGI1GKIoCJycnOD4+xsnJCdrtNi4uLuC6LiaTCcbjMS4uLnB+fi6uIyz3iS4/rrweMQxDWJbFXRNobtgJ0WzshKgatkKrspJ7NiRJQrPZhKZp4vq+qp9XLv/t7u5C13U8e/YMk8lE3DA0GAxgWZa4Aen09BSe54klvKvKE951XTx8+BCdTkdcezYcDtFsNjEajeA4jrjusN/vo9ls4vz8XCznpWkK27Zh2zZevnyJk5OTub1XVF0drrFlJ+yk7tjJ689jJ3SbOnQCsBW2Un+1vWejKAqEYYg4ju/8NElFUaAoCuI4hqqqePXqlZjOy2U60zSRJAk0TcNkMkEURW98nfI6w/Jnx3GQJIm4iSlNUxiGAcdxEASBeDPTNIXneZhMJvB9X9zMZNs2DMO403cKiN6FnRDNxk6IqmErtCorfYL41ZuO7vp55U1HSZJcm6qu3owE3P6UxnLbtJsPoimPpfz/m9uwlduslb9efo3yczfpqZDrpg7vOzt5/fXZST3V4X1nJ6+/Pjupp7q872zl9ddnK/VU5X1f6bBBNE91+IeGnVDdsROi2erQCcBWqP6qtFJ9DY2IiIiIiOgOOGwQEREREdFCVL6MioiIiIiI6C64skFERERERAvBYYOIiIiIiBaCwwYRERERES0Ehw0iIiIiIloIDhtERERERLQQHDaIiIiIiGghOGwQEREREdFCcNggIiIiIqKF4LBBREREREQL8f8B7X1CYDduurAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the 4 frames in a 1x4 grid\n",
    "plt.figure(figsize=(10, 8))\n",
    "for idx in range(state.shape[0]):\n",
    "    plt.subplot(1, 4, idx + 1)\n",
    "    plt.imshow(state[idx, :, :], cmap=\"gray\")\n",
    "    plt.axis(\"off\")  # Turn off axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # self.memory = deque([], maxlen=capacity)\n",
    "        self.memory = []\n",
    "        self.capacity = capacity\n",
    "        self.pointer = 0  # Track the next position to overwrite if needed\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            # If buffer is not full, add new transition normally\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        # If buffer is full, overwrite the oldest element\n",
    "        self.memory[self.pointer] = Transition(*args)\n",
    "        # Update the pointer\n",
    "        self.pointer = int((self.pointer + 1) % self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = Transition(*zip(*batch))\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, filename, hidden_dim=512):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        C, H, W = n_observations\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(n_observations)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "        #Save filename for model\n",
    "        self.filename = filename\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "    # Save a model\n",
    "    def save_model(self):\n",
    "        torch.save(self.state_dict(), './models/' + self.filename + '.pth')\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from transforms import Transforms\n",
    "\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, replace_target_cnt, env, state_space, action_space, \n",
    "                 model_name='enduro_model', gamma=0.99, eps_strt=0.1, \n",
    "                 eps_end=0.001, eps_dec=5e-6, batch_size=32, lr=0.001):\n",
    "        self.env = env\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.batch_size = batch_size\n",
    "        self.GAMMA = gamma\n",
    "        self.LR = lr\n",
    "        self.eps = eps_strt\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_end = eps_end\n",
    "\n",
    "        #Use GPU if available\n",
    "        if torch.backends.mps.is_available():\n",
    "            print('MPS FOUND')\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            print (\"MPS device not found.\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "        #initialize ReplayMemory\n",
    "        self.memory = ReplayMemory(100000)\n",
    "\n",
    "        # After how many training iterations the target network should update\n",
    "        self.replace_target_cnt = replace_target_cnt\n",
    "        self.learn_counter = 0\n",
    "\n",
    "        self.policy_net = DQN(self.state_space, self.action_space, filename=model_name).to(self.device)\n",
    "        self.target_net = DQN(self.state_space, self.action_space, filename=model_name+'target').to(self.device)\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # If pretrained model of the modelname already exists, load it\n",
    "        try:\n",
    "            self.policy_net.load_model('/Users/tariqgeorges/Documents/Riq Coding/Nov 24/game-rl/models/enduro_model.pth')\n",
    "            print('loaded pretrained model')\n",
    "        except:\n",
    "            print('Didnt load model')\n",
    "            pass\n",
    "\n",
    "         # Set target net to be the same as policy net\n",
    "        self.replace_target_net()\n",
    "\n",
    "        #Set optimizer & loss\n",
    "        self.optim = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        # print(f'Batch.state = {batch.state}')\n",
    "        # print(f'Batch.state[0].shape = {batch.state[0].shape}')\n",
    "        state_shape = batch.state[0].shape\n",
    "        #Batch.state[0].shape (4, 84, 84)\n",
    "\n",
    "        # Convert to tensors with correct dimensions\n",
    "        state = torch.tensor(batch.state).view(self.batch_size, -1, state_shape[1], state_shape[2]).float().to(self.device)\n",
    "        action = torch.tensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward = torch.tensor(batch.reward).float().unsqueeze(1).to(self.device)\n",
    "        state_ = torch.tensor(batch.next_state).view(self.batch_size, -1, state_shape[1], state_shape[2]).float().to(self.device)\n",
    "        done = torch.tensor(batch.done).float().unsqueeze(1).to(self.device)\n",
    "\n",
    "        return state, action, reward, state_, done\n",
    "\n",
    "\n",
    "        #------------------------------------------------#\n",
    "        # # Unpack batch of transitions into separate lists for each attribute\n",
    "        # states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # print(f'States {states}, actions {actions}, rewards {rewards}, next_states{next_states}')\n",
    "\n",
    "        # Convert each list to a tensor with the correct dimensions\n",
    "        # state = torch.tensor(batch.state).float().to(self.device)\n",
    "        # action = torch.tensor(batch.action).unsqueeze(1).to(self.device)  # Ensure actions are 2D (batch_size, 1)\n",
    "        # reward = torch.tensor(batch.reward).float().unsqueeze(1).to(self.device)  # Ensure rewards are 2D (batch_size, 1)\n",
    "        # state_ = torch.tensor(batch.next_state).float().to(self.device)\n",
    "        # done = torch.tensor(batch.done).float().unsqueeze(1).to(self.device)  # Ensure dones are 2D (batch_size, 1)\n",
    "\n",
    "\n",
    "        # return state, action, reward, state_, done\n",
    "\n",
    "    # Returns the greedy action according to the policy net\n",
    "    \n",
    "    def greedy_action(self, obs):\n",
    "    # Ensure obs_ is just the raw observation array\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = obs[0]  # If step returns a tuple, get the observation\n",
    "        # print(\"Choosing Greedy Action\")\n",
    "        obs = torch.tensor(obs).float().to(self.device)\n",
    "        obs = obs.unsqueeze(0)\n",
    "        action = self.policy_net(obs).argmax().item()\n",
    "        return action\n",
    "\n",
    "    # Returns an action based on epsilon greedy method\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        # print(\"Choosing Action\")\n",
    "        if random.random() > self.eps:\n",
    "            action = self.greedy_action(obs)\n",
    "        else:\n",
    "            action = random.choice([x for x in range(self.action_space)])\n",
    "        return action\n",
    "    \n",
    "    def replace_target_net(self):\n",
    "        if self.learn_counter % self.replace_target_cnt == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            print('Target network replaced')\n",
    "    \n",
    "    # Decrement epsilon \n",
    "    def dec_eps(self):\n",
    "        self.eps = max(self.eps_end, self.eps - (self.eps - self.eps_end) / self.eps_dec)\n",
    "        \n",
    "    def play_games(self, num_eps, render=True):\n",
    "        # Set network to eval mode\n",
    "        self.policy_net.eval()\n",
    "        self.env = gym.make(\"Enduro-v4\", render_mode=\"human\")\n",
    "        self.env = preprocess_env(self.env)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for i in range(num_eps):\n",
    "            done = False\n",
    "\n",
    "            # Get preprocessed observation from environment\n",
    "            state, _ = self.env.reset()\n",
    "            \n",
    "            score = 0\n",
    "            cnt = 0\n",
    "            while not done:\n",
    "                # Take the greedy action and observe next state\n",
    "                action = self.greedy_action(state)\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "\n",
    "                # Store transition\n",
    "                self.memory.push(state, action, next_state, reward, int(done))\n",
    "\n",
    "                # Calculate score, set next state and obs, and increment counter\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                cnt += 1\n",
    "\n",
    "            # If the score is more than 300, save a gif of that game\n",
    "            if score > 300:\n",
    "                self.save_gif(cnt)\n",
    "\n",
    "            scores.append(score)\n",
    "            print(f'Episode {i}/{num_eps}: \\n\\tScore: {score}\\n\\tAvg score (past 100): {np.mean(scores[-100:])}\\\n",
    "                    \\n\\tEpsilon: {self.eps}\\n\\tSteps made: {cnt}')\n",
    "        \n",
    "        self.env.close()\n",
    "\n",
    "    def learn(self, num_iters=1):\n",
    "        # print('Learning Func')\n",
    "        # Skip learning if there's not enough memory\n",
    "        if self.memory.pointer < self.batch_size:\n",
    "            return \n",
    "\n",
    "        for i in range(num_iters):\n",
    "            # Sample batch\n",
    "            state, action, reward, state_, done = self.sample_batch()\n",
    "\n",
    "            # Calculate the Q-value of the action taken\n",
    "            q_eval = self.policy_net(state).gather(1, action)\n",
    "\n",
    "            # Calculate the best next action value from the target net and detach it from the computation graph\n",
    "            q_next = self.target_net(state_).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "            # Calculate the target Q-value\n",
    "            # (1 - done) ensures q_target is 0 if transition is in a terminal state\n",
    "            q_target = reward + (1 - done) * (self.GAMMA * q_next)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self.loss(q_eval, q_target).to(self.device)\n",
    "\n",
    "            # Perform backward propagation and optimization step\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            # Increment learn_counter (used for epsilon decay and target network updates)\n",
    "            self.learn_counter += 1\n",
    "\n",
    "            # Check if it's time to replace the target network\n",
    "            self.replace_target_net()\n",
    "\n",
    "        # Save the model and decrement epsilon\n",
    "        self.policy_net.save_model()\n",
    "        self.dec_eps()\n",
    "\n",
    "    def save_gif(self, num_transitions):\n",
    "        frames = []\n",
    "        for i in range(self.memory.pointer - num_transitions, self.memory.pointer):\n",
    "            frame = Image.fromarray(self.memory.memory[i].raw_state, mode='RGB')\n",
    "            frames.append(frame)\n",
    "        \n",
    "        frames[0].save('episode.gif', format='GIF', append_images=frames[1:], save_all=True, duration=10, loop=0)\n",
    "    \n",
    "    # Plays num_eps amount of games, while optimizing the model after each episode\n",
    "    def train(self, num_eps=350, render=False):\n",
    "        scores = []\n",
    "        max_score = 0\n",
    "\n",
    "        for i in range(num_eps):\n",
    "            done = False\n",
    "            max_steps = 8000\n",
    "            # Reset environment and preprocess state\n",
    "            state, _ = self.env.reset()\n",
    "            # print(f\"AFTER RESET, STATE SHAPE IS {state.shape}\")\n",
    "            \n",
    "            score = 0\n",
    "            cnt = 0\n",
    "            while not done and cnt < max_steps:\n",
    "                # Take epsilon greedy action\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                # print(f\"AFTER RESET, NEXT STATE IS {obs_.shape}\")\n",
    "\n",
    "                # Preprocess next state and store transition\n",
    "                self.memory.push(state, action, reward, next_state, int(done))\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                cnt += 1\n",
    "\n",
    "            # Maintain record of the max score achieved so far\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "\n",
    "            # Save a gif if episode is best so far\n",
    "            if score > 300 and score >= max_score:\n",
    "                self.save_gif(cnt)\n",
    "\n",
    "            scores.append(score)\n",
    "            print(f'Episode {i}/{num_eps}: \\n\\tScore: {score}\\n\\tAvg score (past 100): {np.mean(scores[-100:])}\\\n",
    "                \\n\\tEpsilon: {self.eps}\\n\\tTransitions added: {cnt}')\n",
    "            \n",
    "            # Train on as many transitions as there have been added in the episode\n",
    "            print(f'Learning x{math.ceil(cnt/self.batch_size)}')\n",
    "            self.learn(math.ceil(cnt/self.batch_size))\n",
    "\n",
    "        self.env.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/qvxbtx2j0930vt590g25785m0000gn/T/ipykernel_51656/2856215985.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained model\n",
      "Target network replaced\n",
      "Episode 0/350: \n",
      "\tScore: 4.0\n",
      "\tAvg score (past 100): 4.0                \n",
      "\tEpsilon: 0.1\n",
      "\tTransitions added: 4418\n",
      "Learning x35\n",
      "Episode 1/350: \n",
      "\tScore: 2.0\n",
      "\tAvg score (past 100): 3.0                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4420\n",
      "Learning x35\n",
      "Episode 2/350: \n",
      "\tScore: 58.0\n",
      "\tAvg score (past 100): 21.333333333333332                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4424\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 3/350: \n",
      "\tScore: 21.0\n",
      "\tAvg score (past 100): 21.25                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4416\n",
      "Learning x35\n",
      "Episode 4/350: \n",
      "\tScore: 36.0\n",
      "\tAvg score (past 100): 24.2                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4434\n",
      "Learning x35\n",
      "Episode 5/350: \n",
      "\tScore: 82.0\n",
      "\tAvg score (past 100): 33.833333333333336                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4432\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 6/350: \n",
      "\tScore: 48.0\n",
      "\tAvg score (past 100): 35.857142857142854                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4408\n",
      "Learning x35\n",
      "Episode 7/350: \n",
      "\tScore: 6.0\n",
      "\tAvg score (past 100): 32.125                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4420\n",
      "Learning x35\n",
      "Episode 8/350: \n",
      "\tScore: 112.0\n",
      "\tAvg score (past 100): 41.0                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4427\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 9/350: \n",
      "\tScore: 77.0\n",
      "\tAvg score (past 100): 44.6                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4469\n",
      "Learning x35\n",
      "Episode 10/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 40.54545454545455                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4416\n",
      "Learning x35\n",
      "Episode 11/350: \n",
      "\tScore: 89.0\n",
      "\tAvg score (past 100): 44.583333333333336                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4455\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 12/350: \n",
      "\tScore: 58.0\n",
      "\tAvg score (past 100): 45.61538461538461                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4404\n",
      "Learning x35\n",
      "Episode 13/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 42.357142857142854                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4415\n",
      "Learning x35\n",
      "Episode 14/350: \n",
      "\tScore: 101.0\n",
      "\tAvg score (past 100): 46.266666666666666                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4437\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 15/350: \n",
      "\tScore: 8.0\n",
      "\tAvg score (past 100): 43.875                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4425\n",
      "Learning x35\n",
      "Episode 16/350: \n",
      "\tScore: 144.0\n",
      "\tAvg score (past 100): 49.76470588235294                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4425\n",
      "Learning x35\n",
      "Episode 17/350: \n",
      "\tScore: 94.0\n",
      "\tAvg score (past 100): 52.22222222222222                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4420\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 18/350: \n",
      "\tScore: 136.0\n",
      "\tAvg score (past 100): 56.63157894736842                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4418\n",
      "Learning x35\n",
      "Episode 19/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 53.8                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4400\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 20/350: \n",
      "\tScore: 16.0\n",
      "\tAvg score (past 100): 52.0                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4428\n",
      "Learning x35\n",
      "Episode 21/350: \n",
      "\tScore: 12.0\n",
      "\tAvg score (past 100): 50.18181818181818                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4414\n",
      "Learning x35\n",
      "Episode 22/350: \n",
      "\tScore: 48.0\n",
      "\tAvg score (past 100): 50.08695652173913                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4448\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 23/350: \n",
      "\tScore: 42.0\n",
      "\tAvg score (past 100): 49.75                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4419\n",
      "Learning x35\n",
      "Episode 24/350: \n",
      "\tScore: 84.0\n",
      "\tAvg score (past 100): 51.12                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4415\n",
      "Learning x35\n",
      "Episode 25/350: \n",
      "\tScore: 31.0\n",
      "\tAvg score (past 100): 50.34615384615385                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4424\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 26/350: \n",
      "\tScore: 101.0\n",
      "\tAvg score (past 100): 52.22222222222222                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4392\n",
      "Learning x35\n",
      "Episode 27/350: \n",
      "\tScore: 34.0\n",
      "\tAvg score (past 100): 51.57142857142857                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4410\n",
      "Learning x35\n",
      "Episode 28/350: \n",
      "\tScore: 94.0\n",
      "\tAvg score (past 100): 53.03448275862069                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4449\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 29/350: \n",
      "\tScore: 131.0\n",
      "\tAvg score (past 100): 55.63333333333333                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4427\n",
      "Learning x35\n",
      "Episode 30/350: \n",
      "\tScore: 75.0\n",
      "\tAvg score (past 100): 56.25806451612903                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4398\n",
      "Learning x35\n",
      "Episode 31/350: \n",
      "\tScore: 75.0\n",
      "\tAvg score (past 100): 56.84375                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4426\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 32/350: \n",
      "\tScore: 61.0\n",
      "\tAvg score (past 100): 56.96969696969697                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4440\n",
      "Learning x35\n",
      "Episode 33/350: \n",
      "\tScore: 195.0\n",
      "\tAvg score (past 100): 61.029411764705884                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4415\n",
      "Learning x35\n",
      "Episode 34/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 59.285714285714285                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4415\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 35/350: \n",
      "\tScore: 49.0\n",
      "\tAvg score (past 100): 59.0                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4450\n",
      "Learning x35\n",
      "Episode 36/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 57.4054054054054                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4444\n",
      "Learning x35\n",
      "Episode 37/350: \n",
      "\tScore: 92.0\n",
      "\tAvg score (past 100): 58.31578947368421                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4402\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 38/350: \n",
      "\tScore: 117.0\n",
      "\tAvg score (past 100): 59.82051282051282                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4443\n",
      "Learning x35\n",
      "Episode 39/350: \n",
      "\tScore: 111.0\n",
      "\tAvg score (past 100): 61.1                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4417\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 40/350: \n",
      "\tScore: 132.0\n",
      "\tAvg score (past 100): 62.829268292682926                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4421\n",
      "Learning x35\n",
      "Episode 41/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 61.333333333333336                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4452\n",
      "Learning x35\n",
      "Episode 42/350: \n",
      "\tScore: 106.0\n",
      "\tAvg score (past 100): 62.372093023255815                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4456\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 43/350: \n",
      "\tScore: 67.0\n",
      "\tAvg score (past 100): 62.47727272727273                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4445\n",
      "Learning x35\n",
      "Episode 44/350: \n",
      "\tScore: 51.0\n",
      "\tAvg score (past 100): 62.22222222222222                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4375\n",
      "Learning x35\n",
      "Episode 45/350: \n",
      "\tScore: 21.0\n",
      "\tAvg score (past 100): 61.32608695652174                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4410\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 46/350: \n",
      "\tScore: 115.0\n",
      "\tAvg score (past 100): 62.46808510638298                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4394\n",
      "Learning x35\n",
      "Episode 47/350: \n",
      "\tScore: 113.0\n",
      "\tAvg score (past 100): 63.520833333333336                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4427\n",
      "Learning x35\n",
      "Episode 48/350: \n",
      "\tScore: 1.0\n",
      "\tAvg score (past 100): 62.244897959183675                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4433\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 49/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 61.0                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4373\n",
      "Learning x35\n",
      "Episode 50/350: \n",
      "\tScore: 4.0\n",
      "\tAvg score (past 100): 59.88235294117647                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4426\n",
      "Learning x35\n",
      "Episode 51/350: \n",
      "\tScore: 40.0\n",
      "\tAvg score (past 100): 59.5                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4394\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 52/350: \n",
      "\tScore: 83.0\n",
      "\tAvg score (past 100): 59.943396226415096                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4429\n",
      "Learning x35\n",
      "Episode 53/350: \n",
      "\tScore: 0.0\n",
      "\tAvg score (past 100): 58.833333333333336                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4438\n",
      "Learning x35\n",
      "Episode 54/350: \n",
      "\tScore: 114.0\n",
      "\tAvg score (past 100): 59.836363636363636                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4435\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 55/350: \n",
      "\tScore: 111.0\n",
      "\tAvg score (past 100): 60.75                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4441\n",
      "Learning x35\n",
      "Episode 56/350: \n",
      "\tScore: 50.0\n",
      "\tAvg score (past 100): 60.56140350877193                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4390\n",
      "Learning x35\n",
      "Episode 57/350: \n",
      "\tScore: 47.0\n",
      "\tAvg score (past 100): 60.327586206896555                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4428\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 58/350: \n",
      "\tScore: 5.0\n",
      "\tAvg score (past 100): 59.389830508474574                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4431\n",
      "Learning x35\n",
      "Episode 59/350: \n",
      "\tScore: 133.0\n",
      "\tAvg score (past 100): 60.61666666666667                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4434\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 60/350: \n",
      "\tScore: 5.0\n",
      "\tAvg score (past 100): 59.704918032786885                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4455\n",
      "Learning x35\n",
      "Episode 61/350: \n",
      "\tScore: 119.0\n",
      "\tAvg score (past 100): 60.66129032258065                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4403\n",
      "Learning x35\n",
      "Episode 62/350: \n",
      "\tScore: 115.0\n",
      "\tAvg score (past 100): 61.523809523809526                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4386\n",
      "Learning x35\n",
      "Target network replaced\n",
      "Episode 63/350: \n",
      "\tScore: 152.0\n",
      "\tAvg score (past 100): 62.9375                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4421\n",
      "Learning x35\n",
      "Episode 64/350: \n",
      "\tScore: 144.0\n",
      "\tAvg score (past 100): 64.18461538461538                \n",
      "\tEpsilon: 0.001\n",
      "\tTransitions added: 4448\n",
      "Learning x35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# STATES = (4, 84, 84) Actions = 9\u001b[39;00m\n\u001b[1;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(replace_target_cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, env\u001b[38;5;241m=\u001b[39menv, state_space\u001b[38;5;241m=\u001b[39mSTATES, action_space\u001b[38;5;241m=\u001b[39mACTIONS, eps_dec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# agent = DQNAgent(replace_target_cnt=100, env=env, state_space=STATES, action_space=ACTIONS, eps_strt=0.9, eps_end=0.05, eps_dec=1000, lr=1e-4, batch_size=128)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# agent.train()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 250\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, num_eps, render)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Train on as many transitions as there have been added in the episode\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning x\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mceil(cnt\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnt\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[10], line 168\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, num_iters)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Sample batch\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     state, action, reward, state_, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Calculate the Q-value of the action taken\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     q_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(state)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action)\n",
      "Cell \u001b[0;32mIn[10], line 65\u001b[0m, in \u001b[0;36mDQNAgent.sample_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39maction)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     64\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mreward)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 65\u001b[0m state_ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, state_shape[\u001b[38;5;241m1\u001b[39m], state_shape[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     66\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mdone)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, action, reward, state_, done\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Enduro-v4\")\n",
    "env = preprocess_env(env)\n",
    "STATES = env.observation_space.shape\n",
    "ACTIONS = env.action_space.n\n",
    "\n",
    "\n",
    "# STATES = (4, 84, 84) Actions = 9\n",
    "agent = DQNAgent(replace_target_cnt=100, env=env, state_space=STATES, action_space=ACTIONS, eps_dec=0.01, batch_size=128)\n",
    "agent.train()\n",
    "\n",
    "# agent = DQNAgent(replace_target_cnt=100, env=env, state_space=STATES, action_space=ACTIONS, eps_strt=0.9, eps_end=0.05, eps_dec=1000, lr=1e-4, batch_size=128)\n",
    "# agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS FOUND\n",
      "loaded pretrained model\n",
      "Target network replaced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/qvxbtx2j0930vt590g25785m0000gn/T/ipykernel_51656/2856215985.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n",
      "2024-11-16 08:34:58.804 Python[51656:1071369] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/19/qvxbtx2j0930vt590g25785m0000gn/T/com.apple.python3.savedState\n",
      "2024-11-16 08:34:59.470 Python[51656:1071369] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(replace_target_cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, env\u001b[38;5;241m=\u001b[39menv, state_space\u001b[38;5;241m=\u001b[39mSTATES, action_space\u001b[38;5;241m=\u001b[39mACTIONS, eps_dec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_games\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 137\u001b[0m, in \u001b[0;36mDQNAgent.play_games\u001b[0;34m(self, num_eps, render)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Take the greedy action and observe next state\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_action(state)\n\u001b[0;32m--> 137\u001b[0m     next_state, reward, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/gymnasium/wrappers/stateful_observation.py:416\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[1;32m    419\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[1;32m    420\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[1;32m    421\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/gymnasium/wrappers/atari_preprocessing.py:162\u001b[0m, in \u001b[0;36mAtariPreprocessing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    159\u001b[0m total_reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_skip):\n\u001b[0;32m--> 162\u001b[0m     _, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m terminated\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Riq Coding/Nov 24/game-rl/atari-rl/lib/python3.9/site-packages/ale_py/env.py:299\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    297\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 299\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    302\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(replace_target_cnt=100, env=env, state_space=STATES, action_space=ACTIONS, eps_dec=0.01)\n",
    "agent.play_games(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
